{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM RAG Tutorial\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/SamHollings/llm_tutorial/blob/main/llm_tutorial_rag.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "This tutorial will give you a simple introduction to how to get started with an LLM to make a simple RAG app.\n",
    "\n",
    "RAG (Retrieval Augmented Generation) allows us to give foundational models local context, without doing expensive fine-tuning and can be done even normal everyday machines like your laptop.\n",
    "The basic idea is that we store documents as vectors in a database. When the user asks a question to the LLM, we can use langchain to first pass that question to the vector database, which retrieves relevant documents (these can be broken up into chunks, given metadata, summarised and various other steps to improve retrieval). The original question and these documents are then passed to the LLM (e.g. Claude) which then gives back the answer. So, in effect the model seems like it knows about what was in the database, e.g. local knowledge about your business, or hobby or whatever, whe in reality, that information was just injected into the prompt just prior to the model seeing it!\n",
    "\n",
    "The main libraries we will use are:\n",
    "- Langchain: which is basically a wrapper around the various LLMs and other tools to make it more consistent (so you can swap say.. OpenAI for Anthropic, easily)\n",
    "- Anthropic: which is the library through which we will access the Claude model (more on why this is chosen below)\n",
    "- ChromaDB: this is a simple vector database, which is a key part of the RAG model.\n",
    "- sentence-transformer: this is an open-source model for embedding text\n",
    "\n",
    "None of the above are \"the best\" tools - they're just examples, and you may whish to use difference embedding models, LLMs, vector databases, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "- **Add documents to docs folder**: First there is a bit of setup. In this tutorial we won't go through how to take arbitrary sources and turn them into text files - that can be covered elsewhere. Instead, simply place some plain text documents ending in \".txt\" in the \"docs\" folder.\n",
    "    - There is a flat text version of the [Goldacre review](https://www.gov.uk/government/publications/better-broader-safer-using-health-data-for-research-and-analysis/better-broader-safer-using-health-data-for-research-and-analysis) already there to get you started\n",
    "- **.env** file: to use the anthropic Claude model you'll need an access token. That can be made here: https://console.anthropic.com. After this you need to copy the env_example file, rename it \".env\" and add in your access token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this forces google collab to install the dependencies\n",
    "if \"google.colab\" in str(get_ipython()):\n",
    "    print(\"Running on Colab\")\n",
    "    !git clone https://github.com/SamHollings/llm_tutorial.git -q\n",
    "    %cd llm_tutorial\n",
    "    !pip install -r requirements.txt -q -q\n",
    "\n",
    "    import src.utils.colab as colab\n",
    "\n",
    "    colab.upload_dot_env_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "import toml\n",
    "from dotenv import load_dotenv\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatAnthropic\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain.schema.document import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from tqdm import tqdm\n",
    "\n",
    "config = toml.load(\"config.toml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(\".env\")\n",
    "\n",
    "# Use variables\n",
    "# os.environ[\"OPENAI_API_KEY\"] = os.getenv('openai_key')\n",
    "os.environ[\"ANTHROPIC_API_KEY\"] = os.getenv(\"anthropic_key\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialise objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a few different types of objects in a RAG pipeline.\n",
    "- **chunk** because LLMs often can only take in relatively small amounts of text, we need to break larger bodies of text into small chunks. For this we use the `text_splitter`. Exactrly how we chunk up the text is an art in itself, and in this example we simple break it into ~1000 character long chunks (a very simple approach!). \n",
    "- **embed**: the `embedding` model (by default we've chosen HuggingFace's \"sentence-transformer\") converts strings of text in the chunks into a vector representation (if you want to learn more about why it does this, have a look into natural language processing theory)\n",
    "- **store**: the `vectorstore` is the database in which we will store and later retrieve the embedded text vectors for each chunk.\n",
    "- **Question and Answer Chain**: the `RetrievalQA` chain is a langchain object which does a few things for us:\n",
    "    - it takes our question and passes it to the `retriever` which in this case submits our question to the `vectorstore`, embeds it, and then returns simply the top 4 nearest chunks (in vector space)\n",
    "    - this is then \"stuffed\" into a new prompt along with your question. The default prompt is something like this:\n",
    "        - `\"using the following documents: {stuffed documents} answer the following question: {question}. Answer:\"`\n",
    "    - this new prompt is then sent off the `llm` - in this case that is the Anthropic Claude model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\PycharmProjects\\llm_tutorial\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "DEV_MODE = True\n",
    "PERSIST_DIRECTORY = \"db\"\n",
    "EMBEDDING_MODEL = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "\n",
    "if DEV_MODE:\n",
    "    PERSIST_DIRECTORY += \"/dev\"\n",
    "\n",
    "embedding = HuggingFaceEmbeddings(\n",
    "    model_name=EMBEDDING_MODEL\n",
    ")  # embedding_functions.DefaultEmbeddingFunction()\n",
    "vectorstore = Chroma(persist_directory=PERSIST_DIRECTORY, embedding_function=embedding)\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
    "llm = ChatAnthropic(anthropic_api_key=os.getenv(\"ANTHROPIC_API_KEY\"))\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Populate Vector Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below loads the text files into the vector database.\n",
    "- first it uses to glob to get a list of all of the text files in \"docs\"\n",
    "- next it converts this into the `Document` class preferred by langchain\n",
    "- the document is run through the `text_splitter` to break it down into manageable chunks\n",
    "- these chunks are added to the `vectorstore` (where they are first run through the `embedding` model prior to insertion into the database).\n",
    "    - the database itself is just a SQLite database - you can even open it and look inside if you go to the db folder.\n",
    "\n",
    "**NOTE**: this cell may take a bit of time to run, as it needs to chew through and embed quite a lot of text. Go away and make a cup of coffee."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (\n",
    "    not DEV_MODE\n",
    "):  # won't populate the database if in dev mode - we can just use what was already loaded.\n",
    "    for text_file_path in tqdm(\n",
    "        glob.glob(\"docs/*.txt\", recursive=True), desc=\"Processing Files\", position=0\n",
    "    ):\n",
    "        with open(text_file_path, \"r\", encoding=\"utf-8\") as text_file:\n",
    "            doc = Document(\n",
    "                page_content=text_file.read(), metadata={\"file_path\": text_file_path}\n",
    "            )\n",
    "            texts = text_splitter.split_documents([doc])\n",
    "            vectorstore.add_documents(documents=texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question and Retrieve\n",
    "Now we can do the fun part - **ask the model questions**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Based on the context provided, here are some key points about RAP (Reproducible Analytical Pipelines) and what's needed to make them work according to Goldacre:\n",
      "\n",
      "- RAPs represent a modern, efficient approach to delivering high quality, reproducible analytics compared to manual processes. They adopt standard practices from software development like good documentation, open source tools like R and Python, flexibility, and extensibility.\n",
      "\n",
      "- RAPs meet criteria like reproducibility, reusability, auditability, efficiency, high quality, and being less prone to error.\n",
      "\n",
      "- RAPs were first developed by the UK's Government Digital Service in 2017 around the core principle of being able to reproduce everything done today at any point in the future.\n",
      "\n",
      "- To make RAPs work, open source languages like R and Python should be used rather than proprietary tools to ensure future access. \n",
      "\n",
      "- RAPs reflect an emphasis on documentation, flexibility, and extensibility - principles important for all data analysis.\n",
      "\n",
      "- RAPs are now a broad movement across UK government departments with extensive training and experience implementing changes in diverse settings.\n"
     ]
    }
   ],
   "source": [
    "question = \"Describe what the Goldacre says about RAP (Reproducible Analytical Pipelines) and what we need to do to make them work.\"\n",
    "\n",
    "answer = qa.run(question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
