{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM RAG Tutorial\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/SamHollings/llm_tutorial/blob/main/llm_tutorial_rag_sources.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "This tutorial will give you a simple introduction to how to make a RAG pipeline which also tells you the source of it's findings.\n",
    "\n",
    "If you haven't see a basic RAG pipeline, it's worth having a look at our [RAG tutorial](llm_tutorial_rag.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "- **Add documents to docs folder**: First there is a bit of setup. In this tutorial we won't go through how to take arbitrary sources and turn them into text files - that can be covered elsewhere. Instead, simply place some plain text documents ending in \".txt\" in the \"docs\" folder.\n",
    "    - There is a flat text version of the [Goldacre review](https://www.gov.uk/government/publications/better-broader-safer-using-health-data-for-research-and-analysis/better-broader-safer-using-health-data-for-research-and-analysis) already there to get you started\n",
    "- **.env** file: to use the anthropic Claude model you'll need an access token. That can be made here: https://console.anthropic.com. After this you need to copy the env_example file, rename it \".env\" and add in your access token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this forces google collab to install the dependencies\n",
    "if \"google.colab\" in str(get_ipython()):\n",
    "    print(\"Running on Colab\")\n",
    "    !git clone https://github.com/SamHollings/llm_tutorial.git -q\n",
    "    %cd llm_tutorial\n",
    "    !pip install -r requirements.txt -q -q\n",
    "\n",
    "    import src.utils.colab as colab\n",
    "\n",
    "    colab.upload_dot_env_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "import toml\n",
    "from dotenv import load_dotenv\n",
    "from langchain.chains import RetrievalQA, LLMChain\n",
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
    "from langchain.chat_models import ChatAnthropic\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema.document import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "config = toml.load(\"config.toml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(\".env\")\n",
    "\n",
    "# Use variables\n",
    "# os.environ[\"OPENAI_API_KEY\"] = os.getenv('openai_key')\n",
    "os.environ[\"ANTHROPIC_API_KEY\"] = os.getenv(\"anthropic_key\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialise objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a few different types of objects in a RAG pipeline.\n",
    "- **chunk** because LLMs often can only take in relatively small amounts of text, we need to break larger bodies of text into small chunks. For this we use the `text_splitter`. Exactrly how we chunk up the text is an art in itself, and in this example we simple break it into ~1000 character long chunks (a very simple approach!). \n",
    "- **embed**: the `embedding` model (by default we've chosen HuggingFace's \"sentence-transformer\") converts strings of text in the chunks into a vector representation (if you want to learn more about why it does this, have a look into natural language processing theory)\n",
    "- **store**: the `vectorstore` is the database in which we will store and later retrieve the embedded text vectors for each chunk.\n",
    "- **Question and Answer Chain**: the `RetrievalQA` chain is a langchain object which does a few things for us:\n",
    "    - it takes our question and passes it to the `retriever` which in this case submits our question to the `vectorstore`, embeds it, and then returns simply the top 4 nearest chunks (in vector space)\n",
    "    - this is then \"stuffed\" into a new prompt along with your question. The default prompt is something like this:\n",
    "        - `\"using the following documents: {stuffed documents} answer the following question: {question}. Answer:\"`\n",
    "    - this new prompt is then sent off the `llm` - in this case that is the Anthropic Claude model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEV_MODE = True\n",
    "PERSIST_DIRECTORY = \"db\"\n",
    "EMBEDDING_MODEL = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "\n",
    "if DEV_MODE:\n",
    "    PERSIST_DIRECTORY += \"/dev\"\n",
    "\n",
    "embedding = HuggingFaceEmbeddings(\n",
    "    model_name=EMBEDDING_MODEL\n",
    ")  # embedding_functions.DefaultEmbeddingFunction()\n",
    "vectorstore = Chroma(persist_directory=PERSIST_DIRECTORY, embedding_function=embedding)\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 20})\n",
    "llm = ChatAnthropic(anthropic_api_key=os.getenv(\"ANTHROPIC_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### System Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To control how the AI retrieves and uses the retrieved documents, we need to create a few prompts.\n",
    "\n",
    "First we have the `SYSTEM_PROMPT`. This is sets the scene for the AI, and can useful for defining some of the \"framework\" you want the AI to follow ([see this useful tweet thread](https://en.rattibha.com/thread/1711716995987894738)). In this instance, the prompt mostly defines the **Role** the AI will play."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = PromptTemplate.from_template(\"\"\"You are a helpful assistant that helps people with their questions. You are not a replacement for human judgement, but you can help humans\\\n",
    "make more informed decisions. If you are asked a question you cannot answer based on your following instructions, you should say so.\\\n",
    "Be concise and professional in your responses.\\n\\n \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stuff Document Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we have the `STUFF_DOCUMENTS_PROMPT`. The core of RAG is taking documents and jamming them into the prompt which is then sent to the LLM. This is the prompt that defines how that is done (along with the `load_qa_with_sources_chain` which we will see shortly.)\n",
    "\n",
    "Here you can see it follows a straightforward format (see examples of other formats [here](https://en.rattibha.com/thread/1711716995987894738))\n",
    "* *Role* - in the `SYSTEM_PROMPT`\n",
    "* *Objective* - using the docs and the question, create an answer with references\n",
    "* *Details* - don't make stuff up, always return sources used.\n",
    "* *Examples* - \"few-shot\" examples - these are made up, but useful for the AI to *understand* how the output should look. Changing these can greatly change how the output looks and how consistent it is.\n",
    "\n",
    "Next we have the part where it assembles the question, and the retrieved documents together, before preping the AI for the final answer. \n",
    "Import to note - that the pipeline we will pass this prompt to, will not just dump the documents one by one into the \"docs\" part of the prompt - using the following prompts we can get it edit what is retrieved from the database, injecting metadata, adding other text (e.g. reference IDs), and we can change what delimiter separates the documents as they are \"stuffed\" into the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can just add prompts together: just add a string to an existing prompt\n",
    "STUFF_DOCUMENTS_PROMPT = SYSTEM_PROMPT+\"\"\"Given the following extracted parts of a long document and a question, create a final answer with references (\"SOURCES\"). \\\n",
    "If you don't know the answer, just say that you don't know. Don't try to make up an answer. \\\n",
    "ALWAYS return a \"SOURCES\" part in your answer.\n",
    "\n",
    "Example 1: \"**RAP** is to be the foundation of analyst training. SOURCES: (goldacre_review.txt)\"\n",
    "Example 2: \"Open source code is a good idea because:\n",
    "* it's cheap (goldacre_review.txt)\n",
    "* it's easy for people to access and use (open_source_guidlines.txt)\n",
    "* it's easy to share (goldacre_review.txt)\n",
    "\n",
    "SOURCES: (goldacre_review.txt, open_source_guidlines.txt)\"\n",
    "\n",
    "QUESTION: {question}\n",
    "=========\n",
    "{docs}\n",
    "=========\n",
    "FINAL ANSWER:\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inject Document Metadata Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INJECT_METADATA_PROMPT = PromptTemplate.from_template(\"{file_path}:\\n{page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load_qa_with_sources_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "stuff_docs_sources_chain = load_qa_with_sources_chain(\n",
    "            llm,\n",
    "            chain_type=\"stuff\",\n",
    "            prompt=STUFF_DOCUMENTS_PROMPT,\n",
    "            document_prompt=INJECT_METADATA_PROMPT,\n",
    "            document_variable_name=\"docs\",\n",
    "            document_separator=\"\\n\\n\",\n",
    "            verbose=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Populate Vector Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below loads the text files into the vector database.\n",
    "- first it uses to glob to get a list of all of the text files in \"docs\"\n",
    "- next it converts this into the `Document` class preferred by langchain\n",
    "- the document is run through the `text_splitter` to break it down into manageable chunks\n",
    "- these chunks are added to the `vectorstore` (where they are first run through the `embedding` model prior to insertion into the database).\n",
    "    - the database itself is just a SQLite database - you can even open it and look inside if you go to the db folder.\n",
    "\n",
    "**NOTE**: this cell may take a bit of time to run, as it needs to chew through and embed quite a lot of text. Go away and make a cup of coffee."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (\n",
    "    not DEV_MODE\n",
    "):  # won't populate the database if in dev mode - we can just use what was already loaded.\n",
    "    for text_file_path in tqdm(\n",
    "        glob.glob(\"docs/*.txt\", recursive=True), desc=\"Processing Files\", position=0\n",
    "    ):\n",
    "        with open(text_file_path, \"r\", encoding=\"utf-8\") as text_file:\n",
    "            doc = Document(\n",
    "                page_content=text_file.read(), metadata={\"file_path\": text_file_path}\n",
    "            )\n",
    "            texts = text_splitter.split_documents([doc])\n",
    "            vectorstore.add_documents(documents=texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question and Retrieve\n",
    "Now we can do the fun part - **ask the model questions**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Expalin the main benefits of Reproducible Analytical Pipelines (RAP)\"\n",
    "\n",
    "docs = retriever.get_relevant_documents(question)\n",
    "\n",
    "results = stuff_docs_sources_chain({\"question\": question,\n",
    "                          \"input_documents\": docs,\n",
    "                          }\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results['output_text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
